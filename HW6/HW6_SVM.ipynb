{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW6_SVM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUQy7XUO8N2R"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import linalg\n",
        "import cvxopt\n",
        "import cvxopt.solvers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.naive_bayes import GaussianNB as NB\n",
        "\n",
        "# libraries to test against own implementation\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC as LSVC\n",
        "\n",
        "cvxopt.solvers.options['show_progress'] = False\n",
        "np.random.seed(760)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-EYdJP4bi0K"
      },
      "source": [
        "## Linear Logistic Regression Class\n",
        "class LinLog():\n",
        "  def __init__(self, epoch=100, rate=1e-3):\n",
        "    self.epoch = epoch\n",
        "    self.rate = rate\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def sigmoid(self,x, der=False):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    weights = np.zeros(x.shape[1])\n",
        "    b_val = 0\n",
        "    n = len(y)\n",
        "    y = (y+1)/2\n",
        "    for i in range(1,self.epoch+1):\n",
        "      for j, x_train in enumerate(x):\n",
        "        sig = self.sigmoid(np.dot(x_train,weights)+b_val)\n",
        "        weights = weights - (self.rate*x_train*(sig-y[j]))/n\n",
        "        b_val = b_val - (self.rate*(sig-y[j]))/n\n",
        "    self.w = weights\n",
        "    self.b = b_val\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.sign(np.dot(x,self.w)+self.b)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVEo-oVmxBA9"
      },
      "source": [
        "## Linear Support Vector Machine Class\n",
        "class LinearSVM():\n",
        "  def __init__(self, epoch=1000, rate=.01, lamb=1000, c=0):\n",
        "    self.epoch = epoch\n",
        "    self.rate = rate\n",
        "    self.lamb = lamb\n",
        "    self.c = c\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "\n",
        "  def fit(self,x,y):\n",
        "    weight = np.zeros(x.shape[1])\n",
        "    b_val = 0\n",
        "    for epoch in range(1, self.epoch+1):\n",
        "      for i, x_test in enumerate(x):\n",
        "        y_pred = np.dot(x_test,weight)\n",
        "        if (y[i]*y_pred < 1-self.c):\n",
        "          weight = weight + self.rate*((y[i]*x_test)-(2*weight/self.lamb))\n",
        "          b_val = b_val - self.rate*y[i]\n",
        "        else:\n",
        "          weight = weight + self.rate*(-2*weight/self.lamb)\n",
        "      weight = weight/len(x)\n",
        "      b_val = b_val/len(x)\n",
        "    self.w = weight\n",
        "    self.b = b_val\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.sign(np.dot(x,self.w)+self.b)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aecZOwQ68bJO"
      },
      "source": [
        "## Kernel Support Vector Machine Class\n",
        "class KernelSVM():\n",
        "  def __init__(self, c, kernel='linear', degree=2, sigma=1):\n",
        "    if kernel == 'linear':\n",
        "      self.kernel = self.linear_kernel\n",
        "      self.prefix = 'Linear'\n",
        "    elif kernel == 'poly':\n",
        "      self.kernel = self.poly_kernel\n",
        "      self.degree = degree\n",
        "      self.prefix = 'Polynomial'\n",
        "    else:\n",
        "      self.kernel = self.gauss_kernel\n",
        "      self.sigma = sigma\n",
        "      self.prefix = 'Gaussian'\n",
        "    self.c = float(c)\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "\n",
        "  def linear_kernel(self, x1, x2):\n",
        "    return np.dot(x1,x2)\n",
        "\n",
        "  def poly_kernel(self, x1, x2):\n",
        "    return (1+np.dot(x1,x2))**self.degree\n",
        "\n",
        "  def gauss_kernel(self, x1, x2):\n",
        "    return np.exp(-linalg.norm(x1-x2)**2 / (2*(self.sigma**2)))\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    y = y.reshape(-1,1)\n",
        "    n, features = x.shape\n",
        "\n",
        "    # calculate kernel elements\n",
        "    K = np.zeros((n,n))\n",
        "    for i in range(n):\n",
        "      for j in range(n):\n",
        "        K[i,j] = self.kernel(x[i],x[j])\n",
        "\n",
        "    # dual can be rewritten to quadratic so we use quadratic solver\n",
        "    # add small values to diagonal to ensure positive definite\n",
        "    P = cvxopt.matrix((np.outer(y,y) * K)+1e-3*np.eye(len(y)))\n",
        "    q = cvxopt.matrix(np.ones(n) * -1)\n",
        "\n",
        "    # 0 <= alpha <= C\n",
        "    G = cvxopt.matrix(np.vstack((-1*np.eye(n),np.eye(n))))\n",
        "    h = cvxopt.matrix(np.hstack((np.zeros(n),np.ones(n)*self.c)))\n",
        "\n",
        "    # Y.T @ alpha = 0\n",
        "    A = cvxopt.matrix(y, (1,n),tc='d')\n",
        "    b = cvxopt.matrix(0.0)\n",
        "\n",
        "    # solve the equation using the given bounds\n",
        "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
        "    alph = np.array(sol['x'])\n",
        "\n",
        "    # support vectors will correspond to nonzero elements\n",
        "    epsilon = 1e-5\n",
        "    sv = (alph > epsilon).flatten()\n",
        "    self.a = alph[sv]\n",
        "    self.sv_x = x[sv]\n",
        "    self.sv_y = y[sv]\n",
        "\n",
        "    # b = A*Y\n",
        "    self.b = 0.0\n",
        "    for i in range(len(self.a)):\n",
        "      self.b += self.sv_y[i]\n",
        "      for j in range(len(self.a)):\n",
        "        self.b -= self.a[j]*self.sv_y[j]*self.kernel(self.sv_x[i],self.sv_x[j])\n",
        "    self.b /= len(self.a)\n",
        "\n",
        "    # w = A*Y*X\n",
        "    if self.kernel == self.linear_kernel:\n",
        "      self.w = np.zeros(features)\n",
        "      for i in range(len(self.a)):\n",
        "        self.w += self.a[i]*self.sv_y[i]*self.sv_x[i]\n",
        "    else:\n",
        "      self.w = None # if nonlinear we cannot calculate weights for hyperplane\n",
        "    return\n",
        "\n",
        "  def predict(self, x):\n",
        "    if self.w is not None:\n",
        "      # y = w^T.x+b\n",
        "      return np.sign(np.dot(x,self.w)+self.b)\n",
        "    else:\n",
        "      # y = sum(A*Y*K(x,x'))+b\n",
        "      y_pred = np.zeros(len(x))\n",
        "      for i in range(len(x)):\n",
        "        temp = 0\n",
        "        for j in range(len(self.a)):\n",
        "          temp += self.a[j]*self.sv_y[j]*self.kernel(x[i], self.sv_x[j])\n",
        "        y_pred[i] = temp\n",
        "      return np.sign(y_pred+self.b)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9rekgRoNAi4"
      },
      "source": [
        "## Kernel Logistic Regression Class\n",
        "class KernLR():\n",
        "  def __init__(self, c=1e-4, iter=10, rate=1e-3, kernel='linear', degree=2, sigma=1.0):\n",
        "    if kernel == 'linear':\n",
        "      self.kernel = self.linear_kernel\n",
        "      self.prefix = 'Linear'\n",
        "    elif kernel == 'poly':\n",
        "      self.kernel = self.poly_kernel\n",
        "      self.degree = degree\n",
        "      self.prefix = 'Polynomial'\n",
        "    else:\n",
        "      self.kernel = self.gauss_kernel\n",
        "      self.sigma = sigma\n",
        "      self.prefix = 'Gaussian'\n",
        "    self.c = float(c)\n",
        "    self.iter = iter\n",
        "    self.rate = rate\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def linear_kernel(self, x1, x2):\n",
        "    return np.dot(x1,x2.T)\n",
        "\n",
        "  def poly_kernel(self, x1, x2):\n",
        "    return (1+np.dot(x1,x2.T))**self.degree\n",
        "\n",
        "  def gauss_kernel(self, x1, x2):\n",
        "    return np.exp(-linalg.norm(x1-x2)**2 / (2*(self.sigma**2)))\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y.reshape(-1,1)\n",
        "    self.alph = np.zeros((x.shape[0],1))\n",
        "    self.k = self.kernel(x,x)\n",
        "\n",
        "    for i in range(self.iter):\n",
        "      self.alph = self.alph - self.rate*(np.dot(self.k,y) + self.c * np.dot(self.k, self.alph))\n",
        "\n",
        "  def predict(self, x):\n",
        "    print((self.y*self.alph).shape)\n",
        "    print(self.kernel(self.x,x).shape)\n",
        "    y_pred = np.array(np.sign(1/(1 + np.exp(-np.dot(self.y*self.alph, self.kernel(self.x,x))))))\n",
        "    return y_pred"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT4GhE1UTNiv"
      },
      "source": [
        "## Neural Network Class\n",
        "class NeuralNet():\n",
        "  def __init__(self, epoch, batch, input_size=2, verbose=2):\n",
        "    self.epoch = epoch\n",
        "    self.batch = batch\n",
        "    self.verbose = verbose\n",
        "    self.input_size = input_size\n",
        "    self.model = self.init_model()\n",
        "\n",
        "  def init_model(self):\n",
        "    model = keras.models.Sequential([\n",
        "                        keras.layers.Dense(64, activation='relu', input_shape=(self.input_size,)),\n",
        "                        keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(),loss=keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "  def fit(self, x, y):\n",
        "    y = keras.utils.to_categorical((y+1)/2)\n",
        "    self.model.fit(x,y,batch_size=self.batch, epochs=self.epoch, verbose=self.verbose)\n",
        "\n",
        "  def predict(self, x):\n",
        "    return -((np.argmin(self.model.predict(x),axis=1)*2)-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49C_c8LCV_Fp"
      },
      "source": [
        "## Methods to generate datasets and to plot decision boundaries\n",
        "def gen_rand(mu, sig):\n",
        "  x0 = np.random.normal(0,sig,size=(750,2))\n",
        "  x0[:,0] += mu\n",
        "  x1 = np.random.normal(0,sig,size=(750,2))\n",
        "  x1[:,0] -= mu\n",
        "  x = np.vstack((x0,x1))\n",
        "  y0 = np.ones((750,1))\n",
        "  y1 = -1*np.ones((750,1))\n",
        "  y = np.vstack((y0,y1))\n",
        "  x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.33, stratify=y)\n",
        "  x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp)\n",
        "  return x_train, y_train, x_test, y_test, x_val, y_val\n",
        "\n",
        "def gen_circ(num=200, factor=0.8, scale=2, noise=None):\n",
        "  x,y = make_circles(n_samples=num, factor=factor, noise=noise)\n",
        "  x = x*scale\n",
        "  y = ((y*2)-1).reshape(-1,1)\n",
        "  x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.33, stratify=y)\n",
        "  x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp)\n",
        "  return x_train, y_train, x_test, y_test, x_val, y_val\n",
        "\n",
        "def gen_canc_data():\n",
        "  data = load_breast_cancer()\n",
        "  x = data.data\n",
        "  y = (data.target*2)-1\n",
        "  x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.33, stratify=y)\n",
        "  x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.5, stratify=y_temp)\n",
        "  return x_train, y_train, x_test, y_test, x_val, y_val\n",
        "\n",
        "\n",
        "def plot_boundary(val,lab,clf,rang,num=100,title=None,nn=False):\n",
        "  x0g = np.linspace(-rang,rang,num)\n",
        "  x1g = np.linspace(-rang,rang,num)\n",
        "  dec = np.zeros((num,num))\n",
        "  if nn:\n",
        "    inputs = np.zeros((num*num,2))\n",
        "    for i in range(num):\n",
        "      for j in range(num):\n",
        "        inputs[i*num+j] = np.array([[x0g[j],x1g[i]]])\n",
        "    dec = clf.predict(inputs)\n",
        "    dec = dec.reshape(num,num)\n",
        "    \n",
        "  else:\n",
        "    for i, x0 in enumerate(x0g):\n",
        "      for j, x1 in enumerate(x1g):\n",
        "        x = np.array([[x0,x1]])\n",
        "        dec[j,i] = np.sign(clf.predict(x))\n",
        "  plt.contourf(x0g,x1g,dec,colors=['blue','red'],alpha=0.5)\n",
        "  plt.scatter(val[:,0],val[:,1],c=lab,cmap='bwr')\n",
        "  if title is not None:\n",
        "    plt.title(title)\n",
        "  plt.show(block=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7mvBo7S6V_V"
      },
      "source": [
        "# Test for each of the classifiers with parameters allowing different test functions\n",
        "\n",
        "def test_linlog(x_train, y_train, x_test, y_test, x_val, y_val,plot=True):\n",
        "  log_clf = LinLog(epoch=1000)\n",
        "  log_clf.fit(x_train,y_train)\n",
        "  y_pred = log_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    plot_boundary(x_test,y_test,log_clf,6,200,\"Linear Log Reg Boundary w/ Test Points\")\n",
        "    print(\"Linear Log Reg Test Error Rate: %f\" %(err*100),flush=True)\n",
        "  return err\n",
        "\n",
        "def test_linsvm(x_train, y_train, x_test, y_test, x_val, y_val,plot=True):\n",
        "  lin_clf = LinearSVM(epoch=1000)\n",
        "  lin_clf.fit(x_train,y_train)\n",
        "  y_pred = lin_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    plot_boundary(x_test,y_test,lin_clf,6,200,\"Linear SVM Boundary w/ Test Points\")\n",
        "    print(\"Linear SVM Test Error Rate: %f\" %(err*100),flush=True)\n",
        "  return err\n",
        "\n",
        "def test_svm(x_train, y_train, x_test, y_test, x_val, y_val,plot=True,kernel='linear',degree=2,sigma=1):\n",
        "  c_vals = np.logspace(-3,5,9)\n",
        "  errs = []\n",
        "  for c in c_vals:\n",
        "    svm_clf = KernelSVM(c=c,kernel=kernel,degree=degree,sigma=sigma)\n",
        "    svm_clf.fit(x_train,y_train)\n",
        "    y_pred = svm_clf.predict(x_val)\n",
        "    errs.append(np.mean(y_pred.flatten() != y_val.flatten()))\n",
        "  c_opt = c_vals[np.argmin(errs)]\n",
        "  svm_clf = KernelSVM(c=c_opt,kernel=kernel,degree=degree,sigma=sigma)\n",
        "  svm_clf.fit(x_train,y_train)\n",
        "  y_pred = svm_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    plot_boundary(x_test,y_test,svm_clf,6,100,\"%s SVM Boundary w/ Test Points\" %(svm_clf.prefix))\n",
        "    print(\"SVM Test Error Rate: %f\" %(err*100), flush=True)\n",
        "  return err, c_opt\n",
        "\n",
        "def test_svc(x_train, y_train, x_test, y_test, x_val, y_val,plot=True,kernel='linear',degree=2):\n",
        "  c_vals = np.logspace(-3,5,9)\n",
        "  errs = []\n",
        "  for c in c_vals:\n",
        "    svc_clf = SVC(C=c, kernel=kernel, degree=degree)\n",
        "    svc_clf.fit(x_train,y_train.flatten())\n",
        "    y_pred = svc_clf.predict(x_val)\n",
        "    errs.append(np.mean(y_pred.flatten() != y_val.flatten()))\n",
        "  c_opt = c_vals[np.argmin(errs)]\n",
        "  svc_clf = SVC(C=c_opt, kernel=kernel, degree=degree)\n",
        "  svc_clf.fit(x_train,y_train.flatten())\n",
        "  y_pred = svc_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    plot_boundary(x_test,y_test,svc_clf,6,100,\"Poly SVM Boundary w/ Test Points\")\n",
        "    print(\"SVM Test Error Rate: %f\" %(err*100), flush=True)\n",
        "  return err, c_opt\n",
        "\n",
        "def test_ker(x_train, y_train, x_test, y_test, x_val, y_val,plot=True,kernel='linear',degree=2,sigma=1):\n",
        "  c_vals = np.logspace(-9,-3,7)\n",
        "  errs = []\n",
        "  for c in c_vals:\n",
        "    log_clf = KernLR(c=c,kernel=kernel,degree=degree,sigma=sigma)\n",
        "    log_clf.fit(x_train, y_train)  \n",
        "    y_pred = log_clf.predict(x_val)\n",
        "    errs.append(np.mean(y_pred.flatten() != y_val.flatten()))\n",
        "  c_opt = c_vals[np.argmin(errs)]\n",
        "  log_clf = KernLR(c=c_opt,kernel=kernel,degree=degree,sigma=sigma)\n",
        "  log_clf.fit(x_train,y_train)\n",
        "  y_pred = log_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    plot_boundary(x_test,y_test,log_clf,6,100,\"%s SVM Boundary w/ Test Points\" %(log_clf.prefix))\n",
        "    print(\"SVM Test Error Rate: %f\" %(err*100), flush=True)\n",
        "  return err, c_opt\n",
        "\n",
        "def test_knn(x_train, y_train, x_test, y_test, x_val, y_val,plot=True):\n",
        "  k_vals = np.array([1,3,5,9])\n",
        "  errs = []\n",
        "  for k in k_vals:\n",
        "    knn_clf = KNN(n_neighbors=k)\n",
        "    knn_clf.fit(x_train,y_train.squeeze())\n",
        "    y_pred = knn_clf.predict(x_val)\n",
        "    errs.append(np.mean(y_pred != y_val.squeeze()))\n",
        "  k_opt = k_vals[np.argmin(errs)]\n",
        "  knn_clf = KNN(n_neighbors=k_opt)\n",
        "  knn_clf.fit(x_train,y_train.squeeze())\n",
        "  err = 1-knn_clf.score(x_test,y_test)\n",
        "  if plot:\n",
        "    print(\"Optimal k-value: %d\" %(k_opt), flush=True)\n",
        "    plot_boundary(x_test,y_test,knn_clf,6,100,\"KNN Boundary w/ Test Points\")\n",
        "    print(\"KNN Test Error Rate: %f\" %(100*err))\n",
        "  return err, k_opt\n",
        "\n",
        "def test_nb(x_train, y_train, x_test, y_test, x_val, y_val,plot=True):\n",
        "  nb_clf = NB()\n",
        "  nb_clf.fit(x_train,y_train.squeeze())\n",
        "  y_pred = nb_clf.predict(x_test)\n",
        "  err = np.mean(y_pred != y_test.squeeze())\n",
        "  if plot:\n",
        "    plot_boundary(x_test, y_test, nb_clf, 6, 100, \"NB Boundary w/ Test Points\")\n",
        "    print(\"NB Test Error Rate: %f\" %(err*100),flush=True)\n",
        "  return err\n",
        "\n",
        "def test_nn(x_train, y_train, x_test, y_test, x_val, y_val,plot=True, input_size=2):\n",
        "  nn_clf = NeuralNet(epoch=50, batch=10, input_size=input_size, verbose=0)\n",
        "  nn_clf.fit(x_train,y_train)\n",
        "  y_pred = nn_clf.predict(x_test)\n",
        "  err = np.mean(y_pred.flatten() != y_test.flatten())\n",
        "  if plot:\n",
        "    nn_clf.model.summary()\n",
        "    plot_boundary(x_test, y_test, nn_clf, 6, 100, \"Neural Net Boundary w/ Test Points\", nn=True)\n",
        "    print(\"Neural Net Test Error Rate %f\" %(err*100),flush=True)\n",
        "  return err"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVr7TNSc_WQ"
      },
      "source": [
        "### Question 2.2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XJbkWSnYEJ0"
      },
      "source": [
        "# working with linear dataset. uncomment tests to run\n",
        "\n",
        "mu = 2.5\n",
        "sig = 1\n",
        "params = gen_rand(mu,sig)\n",
        "\n",
        "#test_linlog(*params);\n",
        "#test_svm(*params,kernel='linear');\n",
        "#test_knn(*params);\n",
        "#test_nb(*params);\n",
        "#test_nn(*params);"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L_TMdy53dby"
      },
      "source": [
        "# running similar tests as section above but with varying values of mu\n",
        "\n",
        "mu_vals = np.arange(1.0,2.2,0.2)\n",
        "sig = 1\n",
        "plot = False\n",
        "\n",
        "linlog_errs = []\n",
        "linsvm_errs = []\n",
        "knn_errs = []\n",
        "nb_errs = []\n",
        "\n",
        "#for mu in mu_vals:\n",
        "#  params = gen_rand(mu,sig)\n",
        "#  linlog_errs.append(test_linlog(*params,False))\n",
        "#  linsvm_errs.append(test_svm(*params,False,kernel='linear')[0])\n",
        "#  knn_errs.append(test_knn(*params,False)[0])\n",
        "#  nb_errs.append(test_nb(*params,False))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap9Ek8a82jXz"
      },
      "source": [
        "if plot:\n",
        "  plt.figure()\n",
        "  plt.plot(mu_vals,linlog_errs,label='LinLog')\n",
        "  plt.plot(mu_vals,linsvm_errs,label='LinSVM')\n",
        "  plt.plot(mu_vals,knn_errs,label='KNN')\n",
        "  plt.plot(mu_vals,nb_errs,label='Naive Bayes')\n",
        "  plt.legend()\n",
        "  plt.title(\"Error Rates for Various Classifiers\")\n",
        "  plt.ylabel(\"Error Rate\")\n",
        "  plt.xlabel(\"mu Value\");"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wRE-o_4dKo1"
      },
      "source": [
        "### Question 2.2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqfEiz-Cob8U"
      },
      "source": [
        "# working with nonlinear (circles) dataset. uncomment tests to run\n",
        "\n",
        "params_nonlin = gen_circ(num=1500,scale=5)\n",
        "\n",
        "#test_linlog(*params_nonlin);\n",
        "#test_svm(*params_nonlin,kernel='linear');\n",
        "#test_svm(*params_nonlin,kernel='rbf');\n",
        "#test_ker(*params_nonlin);\n",
        "#test_knn(*params_nonlin);\n",
        "#test_nn(*params_nonlin);"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_dq2N12dQa7"
      },
      "source": [
        "### Question 2.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MANW1pcChFq"
      },
      "source": [
        "# working with cancer dataset. uncomment tests to run\n",
        "\n",
        "canc_params = gen_canc_data()\n",
        "\n",
        "#print(test_knn(*canc_params,plot=False));\n",
        "#print(test_linlog(*canc_params,plot=False));\n",
        "#print(test_svm(*canc_params,plot=False,kernel='linear'));\n",
        "#print(test_nn(*canc_params,input_size=30,plot=False));\n",
        "#print(test_svc(*canc_params,plot=False,kernel='poly',degree=2));\n",
        "#print(test_svc(*canc_params,plot=False,kernel='poly',degree=3));\n",
        "#print(test_svc(*canc_params,plot=False,kernel='poly',degree=4));\n",
        "#print(test_svc(*canc_params,plot=False,kernel='rbf'));"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKxhMkGChv5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a45eee-10cc-428c-a7b9-5fc0b94254cf"
      },
      "source": [
        "# L1 regualarized linear SVM to extract important features\n",
        "\n",
        "labels = load_breast_cancer().feature_names\n",
        "errs = []\n",
        "c_vals = np.logspace(-3,5,9)\n",
        "for c in c_vals:\n",
        "  lsvc = LSVC(penalty='l1',dual=False,C=c,max_iter=100000)\n",
        "  lsvc.fit(canc_params[0],canc_params[1])\n",
        "  y_pred = lsvc.predict(canc_params[4])\n",
        "  err = np.mean(y_pred != canc_params[5])\n",
        "  errs.append(err)\n",
        "c_opt = c_vals[np.argmin(errs)]\n",
        "print(errs[np.argmin(errs)])\n",
        "lsvc = LSVC(penalty='l1',dual=False,C=c_opt,max_iter=100000)\n",
        "lsvc.fit(canc_params[0],canc_params[1])\n",
        "tar = np.abs(lsvc.coef_) > 3\n",
        "print(labels[tar.flatten()])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "['mean concave points' 'mean symmetry' 'worst smoothness'\n",
            " 'worst concave points' 'worst symmetry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-30SGFa8zk"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}